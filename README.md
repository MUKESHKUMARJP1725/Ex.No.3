## Experiment No:3 
## Title: Comparative Analysis of Prompting Tools Across Different AI Platforms 


### Name: Mukesh Kumar J.P 
### Reg No: 212222223002 
```
Abstract: This report examines the performance, user experience, and response quality of prompting tools across various AI platforms within a specific use case, such as summarizing text or answering technical questions. The study evaluates leading AI systems based on accuracy, coherence, response time, adaptability, and overall usability. 
1.	Introduction With the rise of AI-powered assistants, different platforms provide unique prompting tools that cater to various user needs. This report compares these tools within the context of text summarization and answering technical questions, highlighting their strengths and weaknesses. 
2.	Methodology To conduct this analysis, we selected multiple AI platforms, including OpenAI’s ChatGPT, Google’s Gemini, Anthropic’s Claude, and Meta’s Llama. Each AI system was given identical prompts within defined scenarios: 
•	Summarizing Text: Summarization of a complex article. 
•	Technical Q&A: Answering a software engineering-related question. 
Evaluation criteria included: 
•	Accuracy: How well the response aligns with the expected answer. 
•	Coherence: Logical flow and readability of the response. 
•	Response Time: Speed of generating an answer. 
•	Adaptability: Ability to refine responses based on follow-up queries. 
•	Usability: Ease of use and customization options. 
3.	Results and Comparison 
Feature 	ChatGPT Gemini Claude 	Llama 
Summarization Accuracy High 	Medium High 	Medium 
Technical Q&A Accuracy High 	Medium High 	Low 
Coherence 	High 	High 	Medium Medium 
Response Time 	Fast 	Fast 	Moderate Slow 
Adaptability 	Very High High 	Medium Low 
Usability 	High 	High 	High 	Medium 
4.	Discussion 
•	Summarization Performance: ChatGPT and Claude provided the most concise yet accurate summaries, whereas Gemini often omitted key details. 
•	Technical Q&A: ChatGPT and Claude demonstrated strong accuracy in answering software-related queries, while Llama struggled with more complex questions. 
•	Coherence & Flow: ChatGPT led in this aspect, maintaining clarity and logical structuring across responses. 
•	Response Time: OpenAI’s ChatGPT and Google’s Gemini outperformed others in speed. 
•	Adaptability: ChatGPT excelled in refining responses based on iterative user inputs, making it ideal for in-depth queries. 
•	Usability: All platforms were relatively user-friendly, though Llama had fewer customization options. 
5.	Case Study: Real-World Application To further validate the results, a realworld test was conducted where users across different fields—education, software development, and content creation—utilized these AI models for their tasks. The findings highlighted that: 
•	Educators preferred ChatGPT for generating lesson summaries due to its detailed yet concise output. 
•	Software developers found Claude’s structured responses useful for debugging and code explanations. 
•	Content creators favored Gemini for quick, fluid summaries, but noted occasional inconsistencies. 
6.	Experiment 2: Enhanced Prompting and Contextual Awareness A second experiment was conducted to analyze how well these AI models handle complex, multi-turn interactions requiring deeper contextual awareness. The test involved: 
•	Scenario 1: Analyzing and summarizing a lengthy scientific research paper with domain-specific terminology. 
•	Scenario 2: Providing a step-by-step explanation of a complex coding problem, including debugging suggestions and optimized code improvements. 
Findings: 
•	ChatGPT and Claude demonstrated superior contextual retention and generated in-depth, structured responses. 
•	Gemini provided fast but sometimes inconsistent results when handling long-term contextual queries. 
•	Llama struggled with maintaining coherence in multi-turn interactions. 
7.	Conclusion Based on our analysis, ChatGPT emerges as the most wellrounded AI platform for both summarization and technical Q&A due to its accuracy, adaptability, and response quality. However, specific use cases might benefit from the strengths of other models, such as Claude’s structured answers or Gemini’s fast response times. 
8.	Future Recommendations Further research could involve domain-specific evaluations, real-world user feedback, and integration testing in various industries to refine prompting techniques for enhanced AI interactions. Additionally, examining how these models evolve with updates and newer versions could provide deeper insights into their long-term capabilities and usability. 
``` 


